{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7eee2e0-eb0f-4feb-b6d4-e2a32aa114a3",
   "metadata": {},
   "source": [
    "# Training Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d195e-989c-404a-8a0a-1827291e7ac6",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c929b5ac-aae6-4a68-9c39-0de00d008ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "import torch.quantization as tq\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949fc0d5-27dd-49ac-9e85-4f3ff3f8b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94160390-1c1c-4438-8b89-bad3688163ad",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "- MNIST\n",
    "- train, validation and test-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988180c0-a36c-43a5-8e9a-4837a1a3d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 50000\n",
      "val size: 10000\n",
      "test size: 10000\n",
      "\n",
      "batch size: 64\n",
      "\n",
      "random sample sample:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKJUlEQVR4nO3cvWvddRvH8eu0SZ+DWrFDrdU4Oba1oNipg7iIk5sUB+vgakFcRHfByUHwH3ByriCCQ4sUKgpSHCwV8QkxSrUx1TY5bh+44Yae66s5jeX1mvPJOSanffsbek2m0+m0AKCqtt3uNwDA1iEKAIQoABCiAECIAgAhCgCEKAAQogBALMz6hZPJZDPfBwCbbJZ/q+xJAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgFi43W8A2DyLi4vtzY0bNzbhnfz3HDx4sL156KGHhl7r/PnzQ7vN4EkBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAICbT6XQ60xdOJpv9XvgP2bat//8TM37U/hUj729en/GbN2/O5XVGHThwoL05depUe/PEE0+0N1VVq6ur7c3S0lJ7c+LEifZmfX29vamqeuCBB9qbkc/RLH8GPSkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAxMLtfgP8u0aOuo0cqtvY2Ghv5mn0MBlV7777bnvzzDPPtDcrKyvtTVXVzp0725uR43FXrlxpby5evNjeVFXt2rWrvbl27drQa92KJwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAmExnvIY2cmiNO9ejjz7a3jz99NNDr3X9+vX25uOPP25vPvnkk/ZmxGOPPTa0O336dHvzwgsvtDcjxwS//vrr9mb0IN69997b3rz22mvtzXvvvdfebHWz/HXvSQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAcCWVeu6559qbF198sb1ZW1trb6qqvvrqq/Zmz5497c3i4mJ7s2/fvvbm8OHD7U1V1a+//treLC8vtzfnzp1rb06ePNnejPxeq6pefvnl9uazzz4beq07jSupALSIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABALt/sN8P/df//9Q7tXX321vXn44Yfbm5FjZpcvX25vqqruu+++9ua7775rb3777bf2ZuRQ5OghuL1797Y3Tz755NBrMfa7nfG+6JbmSQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgHMSbg6eeeqq9efvtt4dea2Njo7354IMP2puRo27PP/98e1NV9eGHH7Y3V65caW+Wlpbam+Xl5fbm2LFj7U1V1eHDh9ub69evz+V1fvrpp/Zmq7sTjtuN8KQAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEJPpjFefJpNJ/5sPbEbN63jVmTNn2pvjx4+3N++//357U1X1yy+/tDeLi4vtzdmzZ9ubkydPtjdVVW+++WZ7c/To0fbm0qVL7c3CQv+m5Pbt29ubqqr19fX2Zt++fe3NoUOH2puXXnqpvXnnnXfam6qqu+++u70Z+TmMbO666672pqpq//797c1HH33U3sxyINGTAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEBs6kG8re7EiRPtzenTp9ubb7/9tr05ePBge1NVdezYsfbmiy++aG9OnTrV3szTuXPn2psjR460N5cvX25vVldX25uqqi+//LK9uXDhQnvz448/tjcjx+MeeeSR9qaqamNjo73ZsWNHe7Nz5872ZuS9VY0dpXz99dfbm5WVlVt+jScFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAGJh1i/cvXt3+5s/+OCD7c3IZcKqqs8//7y9efbZZ9ubxx9/vL1ZW1trbxYWZv7V/GMjv9uRq5iHDh1qb6qqbty40d688cYb7c3IJc1PP/20vfnhhx/amzvRmTNnhnbff/99ezPjMej/sXfv3vZmnldSr169OvRat+JJAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACAm0xkvRe3fv7/9zZeXl9ubv/76q72pqtq1a1d78/PPP7c3Kysr7c3vv//e3szTyCG47du3tzejR/5WV1fbm9HDZPOwZ8+eod3S0lJ7M3Jgcn19fS6v88orr7Q3VWPH7UaOKo58xkc/dzdv3mxv3nrrrfbmm2++ueXXeFIAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQAiJkP4h04cKD9ze+55572ZuQwVFXV4uLiXDZb+cBY1djhr9EDbV0jB8aqqnbv3j2X1xo92Nc1evRx5HM0cjxubW2tvRn5bxo58FdVdfXq1aHdVjZy0PPSpUvtzSwHPT0pABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMTMB/Emk8lmv5d/9DrbtvX7trGx0d7M69Da6EG8HTt2tDcjP7uRn8PIAcKqqmvXrrU3f/zxR3sz8nkYOeA4enjvzz//bG9GfuYjRxXn9eePf2aWv+49KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDEljuIB8DmcBAPgBZRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAYmHWL5xOp5v5PgDYAjwpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEH8DRc+Neu3fo2wAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "# class_names = [\"o\", \"ki\", \"su\", \"tsu\", \"na\", \"ha\", \"ma\", \"ya\", \"re\", \"wo\"]\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Grayscale(num_output_channels=1), # Make it Black & White\n",
    "#     transforms.Resize((28, 28)),                 # Shrink to MNIST size\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# class_names = [\n",
    "#     \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "#     \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "# ]\n",
    "\n",
    "# Download MNIST\n",
    "full_train = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split: 50k train / 10k validation\n",
    "train_size = 50_000\n",
    "val_size = 10_000\n",
    "unused_size = len(full_train) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, _ = random_split(\n",
    "    full_train, [train_size, val_size, unused_size]\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def get_calib_loader(num_samples=1000, batch_size=batch_size):\n",
    "    indices = torch.randperm(len(train_dataset))[:num_samples]\n",
    "    calib_dataset = Subset(train_dataset, indices)\n",
    "    calib_loader = DataLoader(calib_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return calib_loader\n",
    "\n",
    "calib_loader = get_calib_loader(100)\n",
    "    \n",
    "\n",
    "print(\"train size:\", len(train_dataset))\n",
    "print(\"val size:\", len(val_dataset))\n",
    "if unused_size > 0:\n",
    "    print(\"unused_size:\", unused_size)\n",
    "print(\"test size:\", len(test_dataset))\n",
    "print(\"\\nbatch size:\", batch_size)\n",
    "\n",
    "print(\"\\nrandom sample sample:\")\n",
    "idx = random.randint(0, len(test_dataset) - 1)\n",
    "img_tensor, label = test_dataset[idx]\n",
    "plt.imshow(img_tensor.squeeze(), cmap=\"gray\")  # squeeze turns (1, 28, 28) to (28, 28) i think\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5097963-1487-417d-85c5-7a2a0eb857a6",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "- The NN-Architecture does not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d96321-774b-473b-a55a-14978bef0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FP32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1, bias=False)  # bias=False because bn1 has its own bias\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.pool1  = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1, bias=False)  # bias=False because bn1 has its own bias\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.pool2  = nn.MaxPool2d(2)\n",
    "\n",
    "        # 16 channels and 28 / 2 / 2 = 7 (two pools)\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bf5cd-a027-4d04-80d5-36b0f5a05a64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generic Training and Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc78fa5c-1cbb-4e18-88ff-ba1194449201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, epoch_num=1):\n",
    "    model.train()  # just sets the mode :)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    loader_with_tqdm = tqdm(loader, desc=f\"Training Epoch {epoch_num}\")\n",
    "    loader_with_tqdm.total = len(loader)\n",
    "\n",
    "    for x, y in loader_with_tqdm:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f7a154-c641-4c4b-ab26-79582715db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(net, loader):\n",
    "    net.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    total_time = 0\n",
    "    avg_time = 0\n",
    "\n",
    "    # loader_with_tqdm = tqdm(enumerate(loader), desc=f\"Evaluating Model\")\n",
    "    # loader_with_tqdm.total = len(loader)\n",
    "\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        start = time.perf_counter()\n",
    "        out = net(x)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        total_time += end - start\n",
    "        avg_time += (end - start - avg_time) / (i + 1)\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    avg_time /= batch_size\n",
    "\n",
    "    avg_latency_ms = avg_time * 1000\n",
    "    throughput = 1 / avg_time\n",
    "    acc = correct / total\n",
    "    \n",
    "    return total_loss / len(loader), acc, total_time, avg_latency_ms, throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf24841-6862-42b0-a499-74f816840994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_mb(model) -> float:\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "        torch.save(model.state_dict(), tmp.name)\n",
    "        size_mb = os.path.getsize(tmp.name) / (1024 ** 2)\n",
    "    os.remove(tmp.name)\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a2c30e-dd37-471f-a470-ef807e494ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, model_name=\"CNN\"):\n",
    "    test_loss, test_acc, total_time, latency, throughput = evaluate(net, test_loader)\n",
    "    test_size = model_size_mb(net)\n",
    "\n",
    "    print(f\"-- Evaluating {model_name} --\")\n",
    "    print(f\"{model_name} evaluation finished in {total_time:.4f}s\")\n",
    "    print(f\"{model_name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{model_name} Model Size: {test_size:.4f}MB\")\n",
    "    print(f\"{model_name} Inference Latency: {latency:.4f}ms\")\n",
    "    print(f\"{model_name} Inference Throughput per second: {throughput:.0f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e8bb03-17bb-49cb-9362-572c07b55c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_avg_latency(net, iterations=10):\n",
    "    total_avg_lat = 0\n",
    "    for _ in range(iterations):\n",
    "        _, _, _, avg, _ = evaluate(net, test_loader)\n",
    "        total_avg_lat += avg\n",
    "    avg_latency_ms = total_avg_lat / iterations\n",
    "    return avg_latency_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333c94d-4436-4c45-818a-8b5709aa159c",
   "metadata": {},
   "source": [
    "## Training\n",
    "- Train once or load if exists (if already trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19c64518-96ee-417d-9963-ef57ea12310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained FP32 model from disk...\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/fp32.pth\"\n",
    "\n",
    "model = FP32().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "val_accuracies = []\n",
    "epoch_train_times = []\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading trained FP32 model from disk...\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "\n",
    "else:\n",
    "    print(\"Training FP32 model...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        _ = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, epoch_num=epoch+1\n",
    "        )\n",
    "        epoch_time = time.time() - start\n",
    "        \n",
    "        _, val_acc, _, _, _ = evaluate(\n",
    "            model, val_loader\n",
    "        )\n",
    "\n",
    "        epoch_train_times.append(epoch_time)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Trained Epoch {epoch+1} in {epoch_time} seconds with val_accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    print(\"Training finished\")\n",
    "\n",
    "    total_training_time = sum(epoch_train_times)\n",
    "    print(f\"Total Time trained: {total_training_time}\")\n",
    "    print(f\"Avg. Time per Epoch: {total_training_time / num_epochs}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"Model saved to:\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d2928-5988-4793-8315-b0d8cab0eb96",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- generic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e06314d-76fa-42c2-a23d-770f41bbb07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ Naive --\n",
      "PTQ Naive evaluation finished in 0.2282s\n",
      "PTQ Naive Test Accuracy: 0.9063\n",
      "PTQ Naive Model Size: 0.3982MB\n",
      "PTQ Naive Inference Latency: 0.0227ms\n",
      "PTQ Naive Inference Throughput per second: 44023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, model_name=\"PTQ Naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4b0cf-a2fe-4480-a07c-ed128cede5bd",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b748b188-8cdd-46d3-a2f4-a61e53b8efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n",
      "10048\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(test_loader) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad47461-ff9f-4664-92b3-d4466645fb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: T-shirt/top\n",
      "Prediction:   T-shirt/top\n",
      "Result:       ✅ Correct\n"
     ]
    }
   ],
   "source": [
    "def visualize_prediction(model, dataset):\n",
    "    idx = random.randint(0, len(dataset) - 1)\n",
    "    img_tensor, label = dataset[idx]\n",
    "    plt.imshow(img_tensor.squeeze(), cmap=\"gray\")  # squeeze turns (1, 28, 28) to (28, 28) i think\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Inference\n",
    "    model.eval()\n",
    "    input_batch = img_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        prediction = output.argmax(dim=1).item()\n",
    "    \n",
    "    pred_name = class_names[prediction]\n",
    "    gt_name   = class_names[label]\n",
    "\n",
    "    print(f\"Actual Label: {gt_name}\")\n",
    "    print(f\"Prediction:   {pred_name}\")\n",
    "    \n",
    "    if prediction == label:\n",
    "        print(\"Result:       ✅ Correct\")\n",
    "    else:\n",
    "        print(\"Result:       ❌ Incorrect\")\n",
    "\n",
    "visualize_prediction(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9dfb2-e940-46b1-8f3b-f017474009b6",
   "metadata": {},
   "source": [
    "# PTQ (Naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eda24ab-66ac-4513-8b7f-a900d1971c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePTQ(FP32):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()  # introduces observers\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81d8f886-9931-48ae-b552-18d124d99c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): Conv2d(\n",
       "    1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(\n",
       "    8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(\n",
       "    8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn2): BatchNorm2d(\n",
       "    16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(\n",
       "    in_features=784, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_ptq_naive = NaivePTQ().to(device)\n",
    "net_ptq_naive.load_state_dict(model.state_dict())\n",
    "net_ptq_naive.eval()\n",
    "\n",
    "net_ptq_naive.qconfig = torch.ao.quantization.default_qconfig\n",
    "net_ptq_naive = torch.ao.quantization.prepare(net_ptq_naive)  # insert observers for callibration / MinMaxObservers\n",
    "net_ptq_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047d4c7-a4e3-4ea0-997d-e2cf7acca249",
   "metadata": {},
   "source": [
    "### Callibrate with evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8213539e-363f-45c7-84c1-3f5ce9d30f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18730862066149712,\n",
       " 0.95,\n",
       " 0.005401400034315884,\n",
       " 0.042198437768092845,\n",
       " 23697.559741325822)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calibration\n",
    "evaluate(net_ptq_naive, calib_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f143951-a53d-484d-bf0f-0afd37975063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): Conv2d(\n",
       "    1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=-1.4034017324447632, max_val=0.9072180390357971)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(\n",
       "    8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-6.621782302856445, max_val=7.239716053009033)\n",
       "  )\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(\n",
       "    8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=-4.817370414733887, max_val=4.71750545501709)\n",
       "  )\n",
       "  (bn2): BatchNorm2d(\n",
       "    16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-8.152894973754883, max_val=9.738616943359375)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(\n",
       "    in_features=784, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-28.33340072631836, max_val=17.824844360351562)\n",
       "  )\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-32.89874267578125, max_val=24.54535484313965)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=1.0)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing all observed min and max values\n",
    "net_ptq_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a028107b-d3b3-4077-99af-df2322e46993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): QuantizedConv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), scale=0.018193857744336128, zero_point=77, padding=(1, 1), bias=False)\n",
       "  (bn1): QuantizedBatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): QuantizedConv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.07507776468992233, zero_point=64, padding=(1, 1), bias=False)\n",
       "  (bn2): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): QuantizedLinear(in_features=784, out_features=128, scale=0.3634507358074188, zero_point=78, qscheme=torch.per_tensor_affine)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.4523157477378845, zero_point=73, qscheme=torch.per_tensor_affine)\n",
       "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual quantization:\n",
    "net_ptq_naive = torch.ao.quantization.convert(net_ptq_naive)\n",
    "net_ptq_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3870e7-949e-43a0-888d-4127232636e0",
   "metadata": {},
   "source": [
    "this results in assymatric quantization: Each layer has a scale and a zero_point to quantized the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855fc66-f312-41d5-ad42-f8b4726cb0de",
   "metadata": {},
   "source": [
    "### Compare weights with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c93810f-5d5a-46ab-ae6b-3fb1f2fe7242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of FT32 baseline:\n",
      "tensor([[[-0.0369, -0.2181,  0.1400],\n",
      "         [-0.1315, -0.0623,  0.3308],\n",
      "         [-0.3941,  0.0820,  0.3094]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "\n",
      "Quantized Weights of naive_ptq:\n",
      "tensor([[[ -9, -54,  35],\n",
      "         [-33, -15,  82],\n",
      "         [-98,  20,  77]]], dtype=torch.int8)\n",
      "\n",
      "\n",
      "Dequantized Weights of naive_ptq:\n",
      "tensor([[[-0.0363, -0.2181,  0.1413],\n",
      "         [-0.1333, -0.0606,  0.3311],\n",
      "         [-0.3957,  0.0808,  0.3109]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weights of FT32 baseline:\")\n",
    "print(model.conv1.weight[0])\n",
    "print(\"\\n\")\n",
    "print(f\"Quantized Weights of naive_ptq:\")\n",
    "print(torch.int_repr(net_ptq_naive.conv1.weight()[0]))\n",
    "print(\"\\n\")\n",
    "print(f\"Dequantized Weights of naive_ptq:\")\n",
    "print(torch.dequantize(net_ptq_naive.conv1.weight()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0feb242-27bb-44a2-857a-519ac516352e",
   "metadata": {},
   "source": [
    "### Evaluate PTQNaive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31cffd83-d903-464a-89d6-a499493b04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ Naive --\n",
      "PTQ Naive evaluation finished in 0.3489s\n",
      "PTQ Naive Test Accuracy: 0.9054\n",
      "PTQ Naive Model Size: 0.1080MB\n",
      "PTQ Naive Inference Latency: 0.0347ms\n",
      "PTQ Naive Inference Throughput per second: 28801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(net_ptq_naive, model_name=\"PTQ Naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d550c4-4a4f-467d-956d-5a752e4d140f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PTQ + Bias Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e114b-c7fb-4d59-80a0-6bdbeb4ffb80",
   "metadata": {},
   "source": [
    "- Bias Correction operates layer-wise\n",
    "- After PTQ a layer computes:\n",
    "$$\n",
    "y_q=W_qx_q+b\n",
    "$$\n",
    "- The Idea for Bias Correction:\n",
    "$$\n",
    "\\mathbb{E}[W_qx_q]\\neq\\mathbb{E}[Wx]\n",
    "$$\n",
    "- fix in bias $b$ of NN:\n",
    "$$\n",
    "b'=b+\\mathbb{E}[Wx]-\\mathbb{E}[W_qx_q]\n",
    "$$\n",
    "- new output:\n",
    "$$\n",
    "y_q=W_qx_q+b'\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1efda2a-c0ec-4a19-a115-8016c4084553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_layer_outputs(net, loader, layers):\n",
    "    outputs = {name: [] for name in layers}\n",
    "\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook(module, inp, out):\n",
    "            outputs[name].append(out.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    for name, module in net.named_modules():\n",
    "        if name in layers:\n",
    "            hooks.append(module.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            net(x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return {k: torch.cat(v, dim=0) for k, v in outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fcdb4c3-71d0-4d69-965a-852a13701e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_correction_pre_convert(fp32_model, prepared_model, calib_loader):\n",
    "    layers = [\n",
    "        name for name, m in fp32_model.named_modules()\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)) and m.bias is not None\n",
    "    ]\n",
    "\n",
    "    fp32_outs = collect_layer_outputs(fp32_model, calib_loader, layers)\n",
    "    prep_outs = collect_layer_outputs(prepared_model, calib_loader, layers)\n",
    "\n",
    "    for name in layers:\n",
    "        fp32 = fp32_outs[name]\n",
    "        prep = prep_outs[name]\n",
    "\n",
    "        if fp32.dim() == 4:   # Conv\n",
    "            delta = (fp32 - prep).mean(dim=(0, 2, 3))\n",
    "        else:                 # Linear\n",
    "            delta = (fp32 - prep).mean(dim=0)\n",
    "\n",
    "        module = dict(prepared_model.named_modules())[name]\n",
    "        module.bias.data += delta.to(module.bias.device)\n",
    "\n",
    "    return prepared_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd71fa58-6db8-4fc7-946b-80c19a7b4093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): Conv2d(\n",
       "    1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(\n",
       "    8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(\n",
       "    8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn2): BatchNorm2d(\n",
       "    16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(\n",
       "    in_features=784, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_bias = NaivePTQ().to(device)\n",
    "net_bias.load_state_dict(model.state_dict())\n",
    "net_bias.eval()\n",
    "\n",
    "net_bias.qconfig = torch.ao.quantization.default_qconfig\n",
    "torch.ao.quantization.prepare(net_bias, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b051481f-125c-419c-9e8d-fa214a205d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FP32(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_ref = FP32().to(device)\n",
    "fp32_ref.load_state_dict(model.state_dict())\n",
    "fp32_ref.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb8495f9-e51e-4d20-9bb7-193a8e888cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_bias = bias_correction_pre_convert(\n",
    "    fp32_ref,\n",
    "    net_bias,\n",
    "    calib_loader\n",
    ")\n",
    "\n",
    "net_bias = torch.ao.quantization.convert(net_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89164ad9-9142-44bd-8590-01f914dd853e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ with Bias Correction --\n",
      "PTQ with Bias Correction evaluation finished in 0.3514s\n",
      "PTQ with Bias Correction Test Accuracy: 0.9054\n",
      "PTQ with Bias Correction Model Size: 0.1080MB\n",
      "PTQ with Bias Correction Inference Latency: 0.0350ms\n",
      "PTQ with Bias Correction Inference Throughput per second: 28593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(net_bias, model_name=\"PTQ with Bias Correction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96027df3-c1bc-491c-a671-93b0f68270f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PTQ + BN Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71b9501a-1be8-4471-b1a6-bbd25a7e53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTQ_BN_Folding(FP32):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)  # (Fusion will replace conv1+bn1+relu1 with a single ConvReLU module / same with conv2+bn2+relu2)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a6ccb74-0edf-48c7-9b81-fba9c5cb2f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights from the trained BN model\n",
    "net_ptq_fold = PTQ_BN_Folding().to(device)\n",
    "net_ptq_fold.load_state_dict(model.state_dict())\n",
    "net_ptq_fold.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6a29bdc-d9fe-479d-9725-5cf61223c4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (bn1): Identity()\n",
       "  (relu1): Identity()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): ConvReLU2d(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (bn2): Identity()\n",
       "  (relu2): Identity()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fusing Conv2d + BatchNorm2d + ReLU\n",
    "torch.ao.quantization.fuse_modules(\n",
    "    net_ptq_fold, \n",
    "    [['conv1', 'bn1', 'relu1'], ['conv2', 'bn2', 'relu2']], \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49035f38-bd9e-4eed-bc55-9d5e1772b1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn1): Identity()\n",
       "  (relu1): Identity()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): ConvReLU2d(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn2): Identity()\n",
       "  (relu2): Identity()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(\n",
       "    in_features=784, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare (Insert Observers)\n",
    "net_ptq_fold.qconfig = torch.ao.quantization.default_qconfig\n",
    "torch.ao.quantization.prepare(net_ptq_fold, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9edba52f-f933-4a31-8ddc-a8d749ffa592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18730862438678741,\n",
       " 0.95,\n",
       " 0.00480940006673336,\n",
       " 0.03757343802135438,\n",
       " 26614.546143785483)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calibrate\n",
    "evaluate(net_ptq_fold, calib_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62d99071-7117-403a-ba9e-ee1e8d42cf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): QuantizedConvReLU2d(1, 8, kernel_size=(3, 3), stride=(1, 1), scale=0.0570056326687336, zero_point=0, padding=(1, 1))\n",
       "  (bn1): Identity()\n",
       "  (relu1): Identity()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): QuantizedConvReLU2d(8, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.07668203115463257, zero_point=0, padding=(1, 1))\n",
       "  (bn2): Identity()\n",
       "  (relu2): Identity()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): QuantizedLinear(in_features=784, out_features=128, scale=0.3634507358074188, zero_point=78, qscheme=torch.per_tensor_affine)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.4523157477378845, zero_point=73, qscheme=torch.per_tensor_affine)\n",
       "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual quantization\n",
    "torch.ao.quantization.convert(net_ptq_fold, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "841a1839-ad20-45f1-8c12-e85b8f158319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ with BN Folding --\n",
      "PTQ with BN Folding evaluation finished in 0.2256s\n",
      "PTQ with BN Folding Test Accuracy: 0.9061\n",
      "PTQ with BN Folding Model Size: 0.1047MB\n",
      "PTQ with BN Folding Inference Latency: 0.0225ms\n",
      "PTQ with BN Folding Inference Throughput per second: 44541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "test(net_ptq_fold, model_name=\"PTQ with BN Folding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b5703-84d3-42f0-b0ae-08e5e49aa5a8",
   "metadata": {},
   "source": [
    "# Mixed Precision PTQ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f654a54-38cc-4b42-b6c8-9284b4b0dce9",
   "metadata": {},
   "source": [
    "## Calculate Layerwise Error / Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11713049-fa16-46f8-89ae-0c74718a4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_sensitivity(fp32_model, quantized_model, loader, num_batches=None):\n",
    "    fp32_model.eval()\n",
    "    quantized_model.eval()\n",
    "    \n",
    "    total_squared_errors = {} \n",
    "    total_counts = {}\n",
    "    \n",
    "    # temp - cleaned after every batch\n",
    "    batch_fp32_out = {}\n",
    "    batch_quant_out = {}\n",
    "    \n",
    "    target_prefixes = (\"conv\", \"fc\")\n",
    "\n",
    "    def make_hook(storage_dict, layer_name):\n",
    "        def hook(module, inp, out):\n",
    "            if hasattr(out, \"is_quantized\") and out.is_quantized:\n",
    "                out = out.dequantize()\n",
    "            storage_dict[layer_name] = out.detach().cpu()\n",
    "        return hook\n",
    "\n",
    "    handles = []\n",
    "    \n",
    "    for name, mod in fp32_model.named_modules():\n",
    "        if name.startswith(target_prefixes):\n",
    "            handles.append(mod.register_forward_hook(make_hook(batch_fp32_out, name)))\n",
    "            \n",
    "    for name, mod in quantized_model.named_modules():\n",
    "        if name.startswith(target_prefixes):\n",
    "            handles.append(mod.register_forward_hook(make_hook(batch_quant_out, name)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(loader):\n",
    "            if num_batches and i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            x = x.to(device)\n",
    "            \n",
    "            batch_fp32_out.clear()\n",
    "            batch_quant_out.clear()\n",
    "            \n",
    "            fp32_model(x)\n",
    "            quantized_model(x)\n",
    "            \n",
    "            for name in batch_fp32_out.keys():\n",
    "                if name in batch_quant_out:\n",
    "                    out_f = batch_fp32_out[name]  # tensor\n",
    "                    out_q = batch_quant_out[name]\n",
    "                    \n",
    "                    batch_squared_error = (out_f - out_q).pow(2).sum().item()\n",
    "                    batch_count = out_f.numel()\n",
    "                    \n",
    "                    total_squared_errors[name] = total_squared_errors.get(name, 0.0) + batch_squared_error\n",
    "                    total_counts[name] = total_counts.get(name, 0) + batch_count\n",
    "\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    final_sensitivity = {}\n",
    "    for name in total_squared_errors:\n",
    "        final_sensitivity[name] = total_squared_errors[name] / total_counts[name]\n",
    "\n",
    "    return final_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c78ea7e9-ad0a-4c44-99b8-f93e5957505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting robust sensitivity analysis...\n",
      "\n",
      "Layer-wise MSE Error (Averaged over batches):\n",
      "conv2: 0.465260\n",
      "conv1: 0.324553\n",
      "fc2: 0.047701\n",
      "fc1: 0.019444\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Starting robust sensitivity analysis...\")\n",
    "sensitivity = get_layer_sensitivity(\n",
    "    model, \n",
    "    net_ptq_fold, \n",
    "    val_loader,\n",
    "    num_batches=20\n",
    ")\n",
    "\n",
    "print(\"\\nLayer-wise MSE Error (Averaged over batches):\")\n",
    "sorted_layers = sorted(sensitivity.items(), key=lambda item: item[1], reverse=True)\n",
    "for name, score in sorted_layers:\n",
    "    print(f\"{name}: {score:.6f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(3, 5))\n",
    "plt.bar(sensitivity.keys(), sensitivity.values(), color='skyblue')\n",
    "plt.title(\"Layerwise Quantization MSE\")\n",
    "plt.ylabel(\"MSE Loss per Batch\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6cf12-7f25-4ee0-944a-dd910050b863",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b818c54-f5bc-43f7-8a81-926d266b6ba5",
   "metadata": {},
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "350afe73-ef3e-47ce-9152-65ade1f73b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating FP32 Baseline --\n",
      "FP32 Baseline evaluation finished in 0.2224s\n",
      "FP32 Baseline Test Accuracy: 0.9063\n",
      "FP32 Baseline Model Size: 0.3982MB\n",
      "FP32 Baseline Inference Latency: 0.0221ms\n",
      "FP32 Baseline Inference Throughput per second: 45175\n",
      "\n",
      "-- Evaluating PTQ Naive --\n",
      "PTQ Naive evaluation finished in 0.3549s\n",
      "PTQ Naive Test Accuracy: 0.9054\n",
      "PTQ Naive Model Size: 0.1080MB\n",
      "PTQ Naive Inference Latency: 0.0353ms\n",
      "PTQ Naive Inference Throughput per second: 28311\n",
      "\n",
      "-- Evaluating PTQ with Bias Correction --\n",
      "PTQ with Bias Correction evaluation finished in 0.3509s\n",
      "PTQ with Bias Correction Test Accuracy: 0.9054\n",
      "PTQ with Bias Correction Model Size: 0.1080MB\n",
      "PTQ with Bias Correction Inference Latency: 0.0349ms\n",
      "PTQ with Bias Correction Inference Throughput per second: 28631\n",
      "\n",
      "-- Evaluating PTQ with BN Folding --\n",
      "PTQ with BN Folding evaluation finished in 0.2223s\n",
      "PTQ with BN Folding Test Accuracy: 0.9061\n",
      "PTQ with BN Folding Model Size: 0.1047MB\n",
      "PTQ with BN Folding Inference Latency: 0.0221ms\n",
      "PTQ with BN Folding Inference Throughput per second: 45208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, model_name=\"FP32 Baseline\")\n",
    "test(net_ptq_naive, model_name=\"PTQ Naive\")\n",
    "test(net_bias, model_name=\"PTQ with Bias Correction\")\n",
    "test(net_ptq_fold, model_name=\"PTQ with BN Folding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ded07fa3-df12-4aad-a913-3a4724ef191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 latency: 0.0224ms\n",
      "PTQ Naive latency: 0.0352ms\n",
      "PTQ Bias Correction latency: 0.0391ms\n",
      "PTQ BN Folding latency: 0.0244ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"FP32 latency: {calculating_avg_latency(model):.4f}ms\")\n",
    "print(f\"PTQ Naive latency: {calculating_avg_latency(net_ptq_naive):.4f}ms\")\n",
    "print(f\"PTQ Bias Correction latency: {calculating_avg_latency(net_bias):.4f}ms\")\n",
    "print(f\"PTQ BN Folding latency: {calculating_avg_latency(net_ptq_fold):.4f}ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
