{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7eee2e0-eb0f-4feb-b6d4-e2a32aa114a3",
   "metadata": {},
   "source": [
    "# Training Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d195e-989c-404a-8a0a-1827291e7ac6",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c929b5ac-aae6-4a68-9c39-0de00d008ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "import torch.quantization as tq\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949fc0d5-27dd-49ac-9e85-4f3ff3f8b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94160390-1c1c-4438-8b89-bad3688163ad",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "- MNIST\n",
    "- train, validation and test-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "988180c0-a36c-43a5-8e9a-4837a1a3d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 4200\n",
      "val size: 5000\n",
      "unused_size: 50800\n",
      "test size: 10000\n",
      "\n",
      "batch size: 64\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download MNIST\n",
    "full_train = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split: 50k train / 10k validation\n",
    "train_size = 4_200\n",
    "val_size = 5_000\n",
    "unused_size = len(full_train) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, _ = random_split(\n",
    "    full_train, [train_size, val_size, unused_size]\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def get_calib_loader(num_samples=1000, batch_size=batch_size):\n",
    "    indices = torch.randperm(len(train_dataset))[:num_samples]\n",
    "    calib_dataset = Subset(train_dataset, indices)\n",
    "    calib_loader = DataLoader(calib_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return calib_loader\n",
    "\n",
    "calib_loader = get_calib_loader(100)\n",
    "    \n",
    "\n",
    "print(\"train size:\", len(train_dataset))\n",
    "print(\"val size:\", len(val_dataset))\n",
    "if unused_size > 0:\n",
    "    print(\"unused_size:\", unused_size)\n",
    "print(\"test size:\", len(test_dataset))\n",
    "print(\"\\nbatch size:\", batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5097963-1487-417d-85c5-7a2a0eb857a6",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "- The NN-Architecture does not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d96321-774b-473b-a55a-14978bef0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FP32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, bias=False)  # bias=False because bn1 has its own bias\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool  = nn.MaxPool2d(2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc1 = nn.Linear(5408, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bf5cd-a027-4d04-80d5-36b0f5a05a64",
   "metadata": {},
   "source": [
    "## Generic Training and Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc78fa5c-1cbb-4e18-88ff-ba1194449201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, epoch_num=1):\n",
    "    model.train()  # just sets the mode :)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    loader_with_tqdm = tqdm(loader, desc=f\"Training Epoch {epoch_num}\")\n",
    "    loader_with_tqdm.total = len(loader)\n",
    "\n",
    "    for x, y in loader_with_tqdm:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f7a154-c641-4c4b-ab26-79582715db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(net, loader):\n",
    "    net.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    total_time = 0\n",
    "    avg_time = 0\n",
    "\n",
    "    # loader_with_tqdm = tqdm(enumerate(loader), desc=f\"Evaluating Model\")\n",
    "    # loader_with_tqdm.total = len(loader)\n",
    "\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        start = time.perf_counter()\n",
    "        out = net(x)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        total_time += end - start\n",
    "        avg_time += (end - start - avg_time) / (i + 1)\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    avg_time /= batch_size\n",
    "\n",
    "    avg_latency_ms = avg_time * 1000\n",
    "    throughput = 1 / avg_time\n",
    "    acc = correct / total\n",
    "    \n",
    "    return total_loss / len(loader), acc, total_time, avg_latency_ms, throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf24841-6862-42b0-a499-74f816840994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_mb(model) -> float:\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "        torch.save(model.state_dict(), tmp.name)\n",
    "        size_mb = os.path.getsize(tmp.name) / (1024 ** 2)\n",
    "    os.remove(tmp.name)\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a2c30e-dd37-471f-a470-ef807e494ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, model_name=\"CNN\"):\n",
    "    test_loss, test_acc, total_time, latency, throughput = evaluate(net, test_loader)\n",
    "    test_size = model_size_mb(net)\n",
    "\n",
    "    print(f\"-- Evaluating {model_name} --\")\n",
    "    print(f\"{model_name} evaluation finished in {total_time:.4f}s\")\n",
    "    print(f\"{model_name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{model_name} Model Size: {test_size:.4f}MB\")\n",
    "    print(f\"{model_name} Inference Latency: {latency:.4f}ms\")\n",
    "    print(f\"{model_name} Inference Throughput per second: {throughput:.0f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94e8bb03-17bb-49cb-9362-572c07b55c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_avg_latency(net, iterations=10):\n",
    "    total_avg_lat = 0\n",
    "    for _ in range(iterations):\n",
    "        _, _, _, avg, _ = evaluate(net, test_loader)\n",
    "        total_avg_lat += avg\n",
    "    avg_latency_ms = total_avg_lat / iterations\n",
    "    return avg_latency_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333c94d-4436-4c45-818a-8b5709aa159c",
   "metadata": {},
   "source": [
    "## Training\n",
    "- Train once or load if exists (if already trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19c64518-96ee-417d-9963-ef57ea12310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FP32 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 49.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 1 in 1.3234822750091553 seconds with val_accuracy: 0.9156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 53.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 2 in 1.2287535667419434 seconds with val_accuracy: 0.9414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 54.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 3 in 1.2178683280944824 seconds with val_accuracy: 0.9348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 54.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 4 in 1.2025132179260254 seconds with val_accuracy: 0.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 54.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 5 in 1.2070283889770508 seconds with val_accuracy: 0.9558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 55.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 6 in 1.1975290775299072 seconds with val_accuracy: 0.9590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 55.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 7 in 1.1900973320007324 seconds with val_accuracy: 0.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 53.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 8 in 1.2292978763580322 seconds with val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 55.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 9 in 1.1916413307189941 seconds with val_accuracy: 0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|███████████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 55.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 10 in 1.1872296333312988 seconds with val_accuracy: 0.9604\n",
      "Training finished\n",
      "Total Time trained: 12.175441026687622\n",
      "Avg. Time per Epoch: 1.2175441026687621\n",
      "Model saved to: ./models/fp32.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/fp32.pth\"\n",
    "\n",
    "model = FP32().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "val_accuracies = []\n",
    "epoch_train_times = []\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading trained FP32 model from disk...\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "\n",
    "else:\n",
    "    print(\"Training FP32 model...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        _ = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, epoch_num=epoch+1\n",
    "        )\n",
    "        epoch_time = time.time() - start\n",
    "        \n",
    "        _, val_acc, _, _, _ = evaluate(\n",
    "            model, val_loader\n",
    "        )\n",
    "\n",
    "        epoch_train_times.append(epoch_time)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Trained Epoch {epoch+1} in {epoch_time} seconds with val_accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    print(\"Training finished\")\n",
    "\n",
    "    total_training_time = sum(epoch_train_times)\n",
    "    print(f\"Total Time trained: {total_training_time}\")\n",
    "    print(f\"Avg. Time per Epoch: {total_training_time / num_epochs}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"Model saved to:\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d2928-5988-4793-8315-b0d8cab0eb96",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- generic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e06314d-76fa-42c2-a23d-770f41bbb07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ Naive --\n",
      "PTQ Naive evaluation finished in 0.5508s\n",
      "PTQ Naive Test Accuracy: 0.9582\n",
      "PTQ Naive Model Size: 2.6512MB\n",
      "PTQ Naive Inference Latency: 0.0548ms\n",
      "PTQ Naive Inference Throughput per second: 18241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, model_name=\"PTQ Naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4b0cf-a2fe-4480-a07c-ed128cede5bd",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b748b188-8cdd-46d3-a2f4-a61e53b8efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "10048\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(test_loader) * batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9dfb2-e940-46b1-8f3b-f017474009b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PTQ (Naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eda24ab-66ac-4513-8b7f-a900d1971c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePTQ(FP32):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()  # introduces observers\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81d8f886-9931-48ae-b552-18d124d99c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): Conv2d(\n",
       "    1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(\n",
       "    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): ReLU()\n",
       "  (fc1): Linear(\n",
       "    in_features=5408, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_ptq_naive = NaivePTQ().to(device)\n",
    "net_ptq_naive.load_state_dict(model.state_dict())\n",
    "net_ptq_naive.eval()\n",
    "\n",
    "net_ptq_naive.qconfig = torch.ao.quantization.default_qconfig\n",
    "net_ptq_naive = torch.ao.quantization.prepare(net_ptq_naive)  # insert observers for callibration / MinMaxObservers\n",
    "net_ptq_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047d4c7-a4e3-4ea0-997d-e2cf7acca249",
   "metadata": {},
   "source": [
    "### Callibrate with evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8213539e-363f-45c7-84c1-3f5ce9d30f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0026195072568953037,\n",
       " 1.0,\n",
       " 0.008057600003667176,\n",
       " 0.06295000002864981,\n",
       " 15885.623503492929)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calibration\n",
    "evaluate(net_ptq_naive, calib_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f143951-a53d-484d-bf0f-0afd37975063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): Conv2d(\n",
       "    1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=-0.80129075050354, max_val=1.168089747428894)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(\n",
       "    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-9.63398551940918, max_val=9.405465126037598)\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): ReLU()\n",
       "  (fc1): Linear(\n",
       "    in_features=5408, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-27.501049041748047, max_val=31.70554542541504)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-16.36876106262207, max_val=20.98222541809082)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=1.0)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing all observed min and max values\n",
    "net_ptq_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a028107b-d3b3-4077-99af-df2322e46993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.015506933443248272, zero_point=52, bias=False)\n",
       "  (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): ReLU()\n",
       "  (fc1): QuantizedLinear(in_features=5408, out_features=128, scale=0.46619367599487305, zero_point=59, qscheme=torch.per_tensor_affine)\n",
       "  (relu2): ReLU()\n",
       "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.2941022515296936, zero_point=56, qscheme=torch.per_tensor_affine)\n",
       "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual quantization:\n",
    "net_ptq_naive = torch.ao.quantization.convert(net_ptq_naive)\n",
    "net_ptq_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3870e7-949e-43a0-888d-4127232636e0",
   "metadata": {},
   "source": [
    "this results in assymatric quantization: Each layer has a scale and a zero_point to quantized the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855fc66-f312-41d5-ad42-f8b4726cb0de",
   "metadata": {},
   "source": [
    "### Compare weights with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c93810f-5d5a-46ab-ae6b-3fb1f2fe7242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of FT32 baseline:\n",
      "tensor([[[ 0.1728,  0.2110, -0.0973],\n",
      "         [-0.2779, -0.2402, -0.2528],\n",
      "         [ 0.1140,  0.1251,  0.2860]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "\n",
      "Quantized Weights of naive_ptq:\n",
      "tensor([[[ 55,  67, -31],\n",
      "         [-88, -76, -81],\n",
      "         [ 36,  40,  91]]], dtype=torch.int8)\n",
      "\n",
      "\n",
      "Dequantized Weights of naive_ptq:\n",
      "tensor([[[ 0.1727,  0.2104, -0.0973],\n",
      "         [-0.2763, -0.2387, -0.2544],\n",
      "         [ 0.1130,  0.1256,  0.2858]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weights of FT32 baseline:\")\n",
    "print(model.conv1.weight[0])\n",
    "print(\"\\n\")\n",
    "print(f\"Quantized Weights of naive_ptq:\")\n",
    "print(torch.int_repr(net_ptq_naive.conv1.weight()[0]))\n",
    "print(\"\\n\")\n",
    "print(f\"Dequantized Weights of naive_ptq:\")\n",
    "print(torch.dequantize(net_ptq_naive.conv1.weight()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0feb242-27bb-44a2-857a-519ac516352e",
   "metadata": {},
   "source": [
    "### Evaluate PTQNaive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31cffd83-d903-464a-89d6-a499493b04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ Naive --\n",
      "PTQ Naive evaluation finished in 0.2320s\n",
      "PTQ Naive Test Accuracy: 0.9582\n",
      "PTQ Naive Model Size: 0.6691MB\n",
      "PTQ Naive Inference Latency: 0.0231ms\n",
      "PTQ Naive Inference Throughput per second: 43305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(net_ptq_naive, model_name=\"PTQ Naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d550c4-4a4f-467d-956d-5a752e4d140f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PTQ + Bias Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e114b-c7fb-4d59-80a0-6bdbeb4ffb80",
   "metadata": {},
   "source": [
    "- Bias Correction operates layer-wise\n",
    "- After PTQ a layer computes:\n",
    "$$\n",
    "y_q=W_qx_q+b\n",
    "$$\n",
    "- The Idea for Bias Correction:\n",
    "$$\n",
    "\\mathbb{E}[W_qx_q]\\neq\\mathbb{E}[Wx]\n",
    "$$\n",
    "- fix in bias $b$ of NN:\n",
    "$$\n",
    "b'=b+\\mathbb{E}[Wx]-\\mathbb{E}[W_qx_q]\n",
    "$$\n",
    "- new output:\n",
    "$$\n",
    "y_q=W_qx_q+b'\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1efda2a-c0ec-4a19-a115-8016c4084553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_layer_outputs(net, loader, layers):\n",
    "    outputs = {name: [] for name in layers}\n",
    "\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook(module, inp, out):\n",
    "            outputs[name].append(out.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    for name, module in net.named_modules():\n",
    "        if name in layers:\n",
    "            hooks.append(module.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            net(x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return {k: torch.cat(v, dim=0) for k, v in outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fcdb4c3-71d0-4d69-965a-852a13701e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_correction_pre_convert(fp32_model, prepared_model, calib_loader):\n",
    "    layers = [\n",
    "        name for name, m in fp32_model.named_modules()\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)) and m.bias is not None\n",
    "    ]\n",
    "\n",
    "    fp32_outs = collect_layer_outputs(fp32_model, calib_loader, layers)\n",
    "    prep_outs = collect_layer_outputs(prepared_model, calib_loader, layers)\n",
    "\n",
    "    for name in layers:\n",
    "        fp32 = fp32_outs[name]\n",
    "        prep = prep_outs[name]\n",
    "\n",
    "        if fp32.dim() == 4:   # Conv\n",
    "            delta = (fp32 - prep).mean(dim=(0, 2, 3))\n",
    "        else:                 # Linear\n",
    "            delta = (fp32 - prep).mean(dim=0)\n",
    "\n",
    "        module = dict(prepared_model.named_modules())[name]\n",
    "        module.bias.data += delta.to(module.bias.device)\n",
    "\n",
    "    return prepared_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd71fa58-6db8-4fc7-946b-80c19a7b4093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): Conv2d(\n",
       "    1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(\n",
       "    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): ReLU()\n",
       "  (fc1): Linear(\n",
       "    in_features=5408, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_bias = NaivePTQ().to(device)\n",
    "net_bias.load_state_dict(model.state_dict())\n",
    "net_bias.eval()\n",
    "\n",
    "net_bias.qconfig = torch.ao.quantization.default_qconfig\n",
    "torch.ao.quantization.prepare(net_bias, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b051481f-125c-419c-9e8d-fa214a205d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FP32(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): ReLU()\n",
       "  (fc1): Linear(in_features=5408, out_features=128, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_ref = FP32().to(device)\n",
    "fp32_ref.load_state_dict(model.state_dict())\n",
    "fp32_ref.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8495f9-e51e-4d20-9bb7-193a8e888cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_bias = bias_correction_pre_convert(\n",
    "    fp32_ref,\n",
    "    net_bias,\n",
    "    calib_loader\n",
    ")\n",
    "\n",
    "net_bias = torch.ao.quantization.convert(net_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89164ad9-9142-44bd-8590-01f914dd853e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ with Bias Correction --\n",
      "PTQ with Bias Correction evaluation finished in 0.2212s\n",
      "PTQ with Bias Correction Test Accuracy: 0.9582\n",
      "PTQ with Bias Correction Model Size: 0.6691MB\n",
      "PTQ with Bias Correction Inference Latency: 0.0220ms\n",
      "PTQ with Bias Correction Inference Throughput per second: 45435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(net_bias, model_name=\"PTQ with Bias Correction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96027df3-c1bc-491c-a671-93b0f68270f5",
   "metadata": {},
   "source": [
    "# PTQ + BN Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71b9501a-1be8-4471-b1a6-bbd25a7e53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTQ_BN_Folding(FP32):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)  # (Fusion will replace conv1+bn1+relu1 with a single ConvReLU module)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a6ccb74-0edf-48c7-9b81-fba9c5cb2f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): ReLU()\n",
       "  (fc1): Linear(in_features=5408, out_features=128, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights from the trained BN model\n",
    "net_ptq_fold = PTQ_BN_Folding().to(device)\n",
    "net_ptq_fold.load_state_dict(model.state_dict())\n",
    "net_ptq_fold.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6a29bdc-d9fe-479d-9725-5cf61223c4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (bn1): Identity()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): Identity()\n",
       "  (fc1): Linear(in_features=5408, out_features=128, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fusing Conv2d + BatchNorm2d + ReLU\n",
    "torch.ao.quantization.fuse_modules(\n",
    "    net_ptq_fold, \n",
    "    [['conv1', 'bn1', 'relu1']], \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49035f38-bd9e-4eed-bc55-9d5e1772b1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn1): Identity()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): Identity()\n",
       "  (fc1): Linear(\n",
       "    in_features=5408, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare (Insert Observers)\n",
    "net_ptq_fold.qconfig = torch.ao.quantization.default_qconfig\n",
    "torch.ao.quantization.prepare(net_ptq_fold, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9edba52f-f933-4a31-8ddc-a8d749ffa592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0016541487129870802,\n",
       " 1.0,\n",
       " 0.006810900056734681,\n",
       " 0.053210156693239696,\n",
       " 18793.404532993023)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calibrate\n",
    "evaluate(net_ptq_fold, calib_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62d99071-7117-403a-ba9e-ee1e8d42cf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): QuantizedConvReLU2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.07639258354902267, zero_point=0)\n",
       "  (bn1): Identity()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): Identity()\n",
       "  (fc1): QuantizedLinear(in_features=5408, out_features=128, scale=0.4036511182785034, zero_point=57, qscheme=torch.per_tensor_affine)\n",
       "  (relu2): ReLU()\n",
       "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.30629250407218933, zero_point=59, qscheme=torch.per_tensor_affine)\n",
       "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual quantization\n",
    "torch.ao.quantization.convert(net_ptq_fold, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "841a1839-ad20-45f1-8c12-e85b8f158319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ with BN Folding --\n",
      "PTQ with BN Folding evaluation finished in 0.1784s\n",
      "PTQ with BN Folding Test Accuracy: 0.9622\n",
      "PTQ with BN Folding Model Size: 0.6672MB\n",
      "PTQ with BN Folding Inference Latency: 0.0178ms\n",
      "PTQ with BN Folding Inference Throughput per second: 56313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "test(net_ptq_fold, model_name=\"PTQ with BN Folding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b5703-84d3-42f0-b0ae-08e5e49aa5a8",
   "metadata": {},
   "source": [
    "# Mixed Precision PTQ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6cf12-7f25-4ee0-944a-dd910050b863",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b818c54-f5bc-43f7-8a81-926d266b6ba5",
   "metadata": {},
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "350afe73-ef3e-47ce-9152-65ade1f73b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating FP32 Baseline --\n",
      "FP32 Baseline evaluation finished in 0.4786s\n",
      "FP32 Baseline Test Accuracy: 0.9627\n",
      "FP32 Baseline Model Size: 2.6512MB\n",
      "FP32 Baseline Inference Latency: 0.0476ms\n",
      "FP32 Baseline Inference Throughput per second: 20995\n",
      "\n",
      "-- Evaluating PTQ Naive --\n",
      "PTQ Naive evaluation finished in 0.2002s\n",
      "PTQ Naive Test Accuracy: 0.9582\n",
      "PTQ Naive Model Size: 0.6691MB\n",
      "PTQ Naive Inference Latency: 0.0199ms\n",
      "PTQ Naive Inference Throughput per second: 50191\n",
      "\n",
      "-- Evaluating PTQ with Bias Correction --\n",
      "PTQ with Bias Correction evaluation finished in 0.1944s\n",
      "PTQ with Bias Correction Test Accuracy: 0.9582\n",
      "PTQ with Bias Correction Model Size: 0.6691MB\n",
      "PTQ with Bias Correction Inference Latency: 0.0193ms\n",
      "PTQ with Bias Correction Inference Throughput per second: 51685\n",
      "\n",
      "-- Evaluating PTQ with BN Folding --\n",
      "PTQ with BN Folding evaluation finished in 0.1892s\n",
      "PTQ with BN Folding Test Accuracy: 0.9622\n",
      "PTQ with BN Folding Model Size: 0.6672MB\n",
      "PTQ with BN Folding Inference Latency: 0.0188ms\n",
      "PTQ with BN Folding Inference Throughput per second: 53097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, model_name=\"FP32 Baseline\")\n",
    "test(net_ptq_naive, model_name=\"PTQ Naive\")\n",
    "test(net_bias, model_name=\"PTQ with Bias Correction\")\n",
    "test(net_ptq_fold, model_name=\"PTQ with BN Folding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ded07fa3-df12-4aad-a913-3a4724ef191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 latency: 0.0507ms\n",
      "PTQ Naive latency: 0.0204ms\n",
      "PTQ Bias Correction latency: 0.0222ms\n",
      "PTQ BN Folding latency: 0.0175ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"FP32 latency: {calculating_avg_latency(model):.4f}ms\")\n",
    "print(f\"PTQ Naive latency: {calculating_avg_latency(net_ptq_naive):.4f}ms\")\n",
    "print(f\"PTQ Bias Correction latency: {calculating_avg_latency(net_bias):.4f}ms\")\n",
    "print(f\"PTQ BN Folding latency: {calculating_avg_latency(net_ptq_fold):.4f}ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
