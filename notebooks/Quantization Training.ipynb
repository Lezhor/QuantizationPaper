{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7eee2e0-eb0f-4feb-b6d4-e2a32aa114a3",
   "metadata": {},
   "source": [
    "# Training Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d195e-989c-404a-8a0a-1827291e7ac6",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c929b5ac-aae6-4a68-9c39-0de00d008ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "import torch.quantization as tq\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949fc0d5-27dd-49ac-9e85-4f3ff3f8b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94160390-1c1c-4438-8b89-bad3688163ad",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "- MNIST\n",
    "- train, validation and test-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "988180c0-a36c-43a5-8e9a-4837a1a3d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train size: 45000\n",
      "val size: 5000\n",
      "test size: 10000\n",
      "\n",
      "batch size: 64\n",
      "\n",
      "random sample sample:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "\n",
    "# class_names = [\n",
    "#     \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "#     \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "# ]\n",
    "\n",
    "# class_names = [\"o\", \"ki\", \"su\", \"tsu\", \"na\", \"ha\", \"ma\", \"ya\", \"re\", \"wo\"]\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Grayscale(num_output_channels=1), # Make it Black & White\n",
    "#     transforms.Resize((28, 28)),                 # Shrink to MNIST size\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # values between -1 and 1\n",
    "])\n",
    "\n",
    "class_names = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Download MNIST\n",
    "full_train = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split: 50k train / 10k validation\n",
    "train_size = 45_000\n",
    "val_size = 5_000\n",
    "unused_size = len(full_train) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, _ = random_split(\n",
    "    full_train, [train_size, val_size, unused_size]\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def get_calib_loader(num_samples=1000, batch_size=batch_size):\n",
    "    indices = torch.randperm(len(train_dataset))[:num_samples]\n",
    "    calib_dataset = Subset(train_dataset, indices)\n",
    "    calib_loader = DataLoader(calib_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return calib_loader\n",
    "\n",
    "calib_loader = get_calib_loader(100)\n",
    "    \n",
    "\n",
    "print(\"train size:\", len(train_dataset))\n",
    "print(\"val size:\", len(val_dataset))\n",
    "if unused_size > 0:\n",
    "    print(\"unused_size:\", unused_size)\n",
    "print(\"test size:\", len(test_dataset))\n",
    "print(\"\\nbatch size:\", batch_size)\n",
    "\n",
    "print(\"\\nrandom sample sample:\")\n",
    "idx = random.randint(0, len(test_dataset) - 1)\n",
    "img_tensor, label = test_dataset[idx]\n",
    "img_tensor = img_tensor * 0.5 + 0.5  # denormalize\n",
    "img_to_show = img_tensor.permute(1, 2, 0).numpy()\n",
    "plt.imshow(img_to_show)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5097963-1487-417d-85c5-7a2a0eb857a6",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "- The NN-Architecture does not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d96321-774b-473b-a55a-14978bef0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FP32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)  # bias=False because bn1 has its own bias\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.pool1  = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)  # bias=False because bn1 has its own bias\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.pool2  = nn.MaxPool2d(2)\n",
    "\n",
    "        # 64 channels and 32 / 2 / 2 = 8 (two pools)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bf5cd-a027-4d04-80d5-36b0f5a05a64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generic Training and Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc78fa5c-1cbb-4e18-88ff-ba1194449201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, epoch_num=1):\n",
    "    model.train()  # just sets the mode :)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    loader_with_tqdm = tqdm(loader, desc=f\"Training Epoch {epoch_num}\")\n",
    "    loader_with_tqdm.total = len(loader)\n",
    "\n",
    "    for x, y in loader_with_tqdm:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f7a154-c641-4c4b-ab26-79582715db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(net, loader):\n",
    "    net.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    total_time = 0\n",
    "    avg_time = 0\n",
    "\n",
    "    # loader_with_tqdm = tqdm(enumerate(loader), desc=f\"Evaluating Model\")\n",
    "    # loader_with_tqdm.total = len(loader)\n",
    "\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        start = time.perf_counter()\n",
    "        out = net(x)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        total_time += end - start\n",
    "        avg_time += (end - start - avg_time) / (i + 1)\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    avg_time /= batch_size\n",
    "\n",
    "    avg_latency_ms = avg_time * 1000\n",
    "    throughput = 1 / avg_time\n",
    "    acc = correct / total\n",
    "    \n",
    "    return total_loss / len(loader), acc, total_time, avg_latency_ms, throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf24841-6862-42b0-a499-74f816840994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_mb(model) -> float:\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "        torch.save(model.state_dict(), tmp.name)\n",
    "        size_mb = os.path.getsize(tmp.name) / (1024 ** 2)\n",
    "    os.remove(tmp.name)\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a2c30e-dd37-471f-a470-ef807e494ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, model_name=\"CNN\"):\n",
    "    test_loss, test_acc, total_time, latency, throughput = evaluate(net, test_loader)\n",
    "    test_size = model_size_mb(net)\n",
    "\n",
    "    print(f\"-- Evaluating {model_name} --\")\n",
    "    print(f\"{model_name} evaluation finished in {total_time:.4f}s\")\n",
    "    print(f\"{model_name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{model_name} Model Size: {test_size:.4f}MB\")\n",
    "    print(f\"{model_name} Inference Latency: {latency:.4f}ms\")\n",
    "    print(f\"{model_name} Inference Throughput per second: {throughput:.0f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e8bb03-17bb-49cb-9362-572c07b55c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_avg_latency(net, iterations=10):\n",
    "    total_avg_lat = 0\n",
    "    for _ in range(iterations):\n",
    "        _, _, _, avg, _ = evaluate(net, test_loader)\n",
    "        total_avg_lat += avg\n",
    "    avg_latency_ms = total_avg_lat / iterations\n",
    "    return avg_latency_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333c94d-4436-4c45-818a-8b5709aa159c",
   "metadata": {},
   "source": [
    "## Training\n",
    "- Train once or load if exists (if already trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19c64518-96ee-417d-9963-ef57ea12310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FP32 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████████████████████████████████████████████████████████| 704/704 [00:26<00:00, 26.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 1 in 26.687149047851562 seconds with val_accuracy: 0.5936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████████████████████████████████████████████████████████| 704/704 [00:27<00:00, 25.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 2 in 27.606303930282593 seconds with val_accuracy: 0.6824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████████████████████████████████████████████████████████| 704/704 [00:26<00:00, 26.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 3 in 26.3345890045166 seconds with val_accuracy: 0.6748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████████████████████████████████████████████████████████| 704/704 [00:26<00:00, 26.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 4 in 26.35403561592102 seconds with val_accuracy: 0.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████████████████████████████████████████████████████████| 704/704 [00:26<00:00, 26.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 5 in 26.575404167175293 seconds with val_accuracy: 0.7080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████████████████████████████████████████████████████████| 704/704 [00:26<00:00, 26.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 6 in 26.53761386871338 seconds with val_accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████████████████████████████████████████████████████████| 704/704 [00:26<00:00, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 7 in 26.82417583465576 seconds with val_accuracy: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████████████████████████████████████████████████████████| 704/704 [00:26<00:00, 26.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 8 in 26.43712043762207 seconds with val_accuracy: 0.7216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████████████████████████████████████████████████████████| 704/704 [00:27<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 9 in 27.207009315490723 seconds with val_accuracy: 0.7208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:29<00:00, 23.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 10 in 29.752137184143066 seconds with val_accuracy: 0.7166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:29<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 11 in 29.254241943359375 seconds with val_accuracy: 0.7288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:28<00:00, 25.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 12 in 28.10904836654663 seconds with val_accuracy: 0.7198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:28<00:00, 25.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 13 in 28.02737593650818 seconds with val_accuracy: 0.7156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:27<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 14 in 27.527302980422974 seconds with val_accuracy: 0.7252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:27<00:00, 25.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 15 in 27.554075241088867 seconds with val_accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:28<00:00, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 16 in 28.25969386100769 seconds with val_accuracy: 0.7158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:27<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 17 in 27.519160747528076 seconds with val_accuracy: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:27<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 18 in 27.713924884796143 seconds with val_accuracy: 0.7226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:28<00:00, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 19 in 28.203318119049072 seconds with val_accuracy: 0.7128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|█████████████████████████████████████████████████████████████| 704/704 [00:28<00:00, 25.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Epoch 20 in 28.02607226371765 seconds with val_accuracy: 0.7176\n",
      "Training finished\n",
      "Total Time trained: 550.5097527503967\n",
      "Avg. Time per Epoch: 27.525487637519838\n",
      "Model saved to: ./models/fp32.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/fp32.pth\"\n",
    "\n",
    "model = FP32().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "val_accuracies = []\n",
    "epoch_train_times = []\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading trained FP32 model from disk...\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "\n",
    "else:\n",
    "    print(\"Training FP32 model...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        _ = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, epoch_num=epoch+1\n",
    "        )\n",
    "        epoch_time = time.time() - start\n",
    "        \n",
    "        _, val_acc, _, _, _ = evaluate(\n",
    "            model, val_loader\n",
    "        )\n",
    "\n",
    "        epoch_train_times.append(epoch_time)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Trained Epoch {epoch+1} in {epoch_time} seconds with val_accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    print(\"Training finished\")\n",
    "\n",
    "    total_training_time = sum(epoch_train_times)\n",
    "    print(f\"Total Time trained: {total_training_time}\")\n",
    "    print(f\"Avg. Time per Epoch: {total_training_time / num_epochs}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"Model saved to:\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d2928-5988-4793-8315-b0d8cab0eb96",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- generic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e06314d-76fa-42c2-a23d-770f41bbb07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ Naive --\n",
      "PTQ Naive evaluation finished in 1.3901s\n",
      "PTQ Naive Test Accuracy: 0.7191\n",
      "PTQ Naive Model Size: 2.0857MB\n",
      "PTQ Naive Inference Latency: 0.1383ms\n",
      "PTQ Naive Inference Throughput per second: 7228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, model_name=\"PTQ Naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4b0cf-a2fe-4480-a07c-ed128cede5bd",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b748b188-8cdd-46d3-a2f4-a61e53b8efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704\n",
      "10048\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(test_loader) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fad47461-ff9f-4664-92b3-d4466645fb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: horse\n",
      "Prediction:   horse\n",
      "Result:       ✅ Correct\n"
     ]
    }
   ],
   "source": [
    "def visualize_prediction(model, dataset):\n",
    "    idx = random.randint(0, len(dataset) - 1)\n",
    "    img_tensor, label = dataset[idx]\n",
    "\n",
    "    img_display = img_tensor.clone()\n",
    "    img_display = img_display * 0.5 + 0.5  # denormalize\n",
    "    img_display = img_display.permute(1, 2, 0).numpy()\n",
    "    plt.imshow(img_display)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Inference\n",
    "    model.eval()\n",
    "    input_batch = img_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        prediction = output.argmax(dim=1).item()\n",
    "    \n",
    "    pred_name = class_names[prediction]\n",
    "    gt_name   = class_names[label]\n",
    "\n",
    "    print(f\"Actual Label: {gt_name}\")\n",
    "    print(f\"Prediction:   {pred_name}\")\n",
    "    \n",
    "    if prediction == label:\n",
    "        print(\"Result:       ✅ Correct\")\n",
    "    else:\n",
    "        print(\"Result:       ❌ Incorrect\")\n",
    "\n",
    "visualize_prediction(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9dfb2-e940-46b1-8f3b-f017474009b6",
   "metadata": {},
   "source": [
    "# PTQ (Naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2eda24ab-66ac-4513-8b7f-a900d1971c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePTQ(FP32):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()  # introduces observers\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "81d8f886-9931-48ae-b552-18d124d99c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): Conv2d(\n",
       "    3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(\n",
       "    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(\n",
       "    32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn2): BatchNorm2d(\n",
       "    64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(\n",
       "    in_features=4096, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_ptq_naive = NaivePTQ().to(device)\n",
    "net_ptq_naive.load_state_dict(model.state_dict())\n",
    "net_ptq_naive.eval()\n",
    "\n",
    "net_ptq_naive.qconfig = torch.ao.quantization.default_qconfig\n",
    "net_ptq_naive = torch.ao.quantization.prepare(net_ptq_naive)  # insert observers for callibration / MinMaxObservers\n",
    "net_ptq_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047d4c7-a4e3-4ea0-997d-e2cf7acca249",
   "metadata": {},
   "source": [
    "### Callibrate with evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8213539e-363f-45c7-84c1-3f5ce9d30f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1007937379181385,\n",
       " 0.96,\n",
       " 0.017298099934123456,\n",
       " 0.1351414057353395,\n",
       " 7399.65663786565)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calibration\n",
    "evaluate(net_ptq_naive, calib_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2f143951-a53d-484d-bf0f-0afd37975063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): Conv2d(\n",
       "    3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=-2.6395344734191895, max_val=2.47123122215271)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(\n",
       "    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-9.967764854431152, max_val=10.926153182983398)\n",
       "  )\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(\n",
       "    32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=-34.853912353515625, max_val=19.176321029663086)\n",
       "  )\n",
       "  (bn2): BatchNorm2d(\n",
       "    64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-12.930061340332031, max_val=9.227407455444336)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(\n",
       "    in_features=4096, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-91.3305892944336, max_val=35.9821662902832)\n",
       "  )\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-44.27003860473633, max_val=33.46914291381836)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-1.0, max_val=1.0)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing all observed min and max values\n",
    "net_ptq_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a028107b-d3b3-4077-99af-df2322e46993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): QuantizedConv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.04024224728345871, zero_point=66, padding=(1, 1), bias=False)\n",
       "  (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.4254349172115326, zero_point=82, padding=(1, 1), bias=False)\n",
       "  (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): QuantizedLinear(in_features=4096, out_features=128, scale=1.00246262550354, zero_point=91, qscheme=torch.per_tensor_affine)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.6121195554733276, zero_point=72, qscheme=torch.per_tensor_affine)\n",
       "  (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual quantization:\n",
    "net_ptq_naive = torch.ao.quantization.convert(net_ptq_naive)\n",
    "net_ptq_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3870e7-949e-43a0-888d-4127232636e0",
   "metadata": {},
   "source": [
    "this results in assymatric quantization: Each layer has a scale and a zero_point to quantized the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855fc66-f312-41d5-ad42-f8b4726cb0de",
   "metadata": {},
   "source": [
    "### Compare weights with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c93810f-5d5a-46ab-ae6b-3fb1f2fe7242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of FT32 baseline:\n",
      "tensor([[[ 0.0976,  0.0690, -0.2166],\n",
      "         [ 0.0373,  0.2020, -0.2204],\n",
      "         [ 0.2167,  0.1306, -0.2251]],\n",
      "\n",
      "        [[ 0.1631, -0.0960, -0.0245],\n",
      "         [ 0.1097, -0.1493,  0.0405],\n",
      "         [-0.1509, -0.2385,  0.0964]],\n",
      "\n",
      "        [[-0.2127, -0.0397,  0.2688],\n",
      "         [-0.1420, -0.1849,  0.2435],\n",
      "         [-0.0736,  0.0536,  0.1556]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "\n",
      "Quantized Weights of naive_ptq:\n",
      "tensor([[[ 16,  11, -36],\n",
      "         [  6,  33, -36],\n",
      "         [ 36,  21, -37]],\n",
      "\n",
      "        [[ 27, -16,  -4],\n",
      "         [ 18, -24,   7],\n",
      "         [-25, -39,  16]],\n",
      "\n",
      "        [[-35,  -7,  44],\n",
      "         [-23, -30,  40],\n",
      "         [-12,   9,  26]]], dtype=torch.int8)\n",
      "\n",
      "\n",
      "Dequantized Weights of naive_ptq:\n",
      "tensor([[[ 0.0976,  0.0671, -0.2195],\n",
      "         [ 0.0366,  0.2013, -0.2195],\n",
      "         [ 0.2195,  0.1281, -0.2256]],\n",
      "\n",
      "        [[ 0.1647, -0.0976, -0.0244],\n",
      "         [ 0.1098, -0.1464,  0.0427],\n",
      "         [-0.1525, -0.2378,  0.0976]],\n",
      "\n",
      "        [[-0.2134, -0.0427,  0.2683],\n",
      "         [-0.1403, -0.1830,  0.2439],\n",
      "         [-0.0732,  0.0549,  0.1586]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weights of FT32 baseline:\")\n",
    "print(model.conv1.weight[0])\n",
    "print(\"\\n\")\n",
    "print(f\"Quantized Weights of naive_ptq:\")\n",
    "print(torch.int_repr(net_ptq_naive.conv1.weight()[0]))\n",
    "print(\"\\n\")\n",
    "print(f\"Dequantized Weights of naive_ptq:\")\n",
    "print(torch.dequantize(net_ptq_naive.conv1.weight()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0feb242-27bb-44a2-857a-519ac516352e",
   "metadata": {},
   "source": [
    "### Evaluate PTQNaive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "31cffd83-d903-464a-89d6-a499493b04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ Naive --\n",
      "PTQ Naive evaluation finished in 0.4887s\n",
      "PTQ Naive Test Accuracy: 0.7142\n",
      "PTQ Naive Model Size: 0.5307MB\n",
      "PTQ Naive Inference Latency: 0.0486ms\n",
      "PTQ Naive Inference Throughput per second: 20562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(net_ptq_naive, model_name=\"PTQ Naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d550c4-4a4f-467d-956d-5a752e4d140f",
   "metadata": {},
   "source": [
    "# PTQ + Bias Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e114b-c7fb-4d59-80a0-6bdbeb4ffb80",
   "metadata": {},
   "source": [
    "- Bias Correction operates layer-wise\n",
    "- After PTQ a layer computes:\n",
    "$$\n",
    "y_q=W_qx_q+b\n",
    "$$\n",
    "- The Idea for Bias Correction:\n",
    "$$\n",
    "\\mathbb{E}[W_qx_q]\\neq\\mathbb{E}[Wx]\n",
    "$$\n",
    "- fix in bias $b$ of NN:\n",
    "$$\n",
    "b'=b+\\mathbb{E}[Wx]-\\mathbb{E}[W_qx_q]\n",
    "$$\n",
    "- new output:\n",
    "$$\n",
    "y_q=W_qx_q+b'\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c1efda2a-c0ec-4a19-a115-8016c4084553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_layer_outputs(net, loader, layers):\n",
    "    outputs = {name: [] for name in layers}\n",
    "\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook(module, inp, out):\n",
    "            outputs[name].append(out.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    for name, module in net.named_modules():\n",
    "        if name in layers:\n",
    "            hooks.append(module.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            net(x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return {k: torch.cat(v, dim=0) for k, v in outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7fcdb4c3-71d0-4d69-965a-852a13701e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_correction_pre_convert(fp32_model, prepared_model, calib_loader):\n",
    "    layers = [\n",
    "        name for name, m in fp32_model.named_modules()\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)) and m.bias is not None\n",
    "    ]\n",
    "\n",
    "    fp32_outs = collect_layer_outputs(fp32_model, calib_loader, layers)\n",
    "    prep_outs = collect_layer_outputs(prepared_model, calib_loader, layers)\n",
    "\n",
    "    for name in layers:\n",
    "        fp32 = fp32_outs[name]\n",
    "        prep = prep_outs[name]\n",
    "\n",
    "        if fp32.dim() == 4:   # Conv\n",
    "            delta = (fp32 - prep).mean(dim=(0, 2, 3))\n",
    "        else:                 # Linear\n",
    "            delta = (fp32 - prep).mean(dim=0)\n",
    "\n",
    "        module = dict(prepared_model.named_modules())[name]\n",
    "        module.bias.data += delta.to(module.bias.device)\n",
    "\n",
    "    return prepared_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dd71fa58-6db8-4fc7-946b-80c19a7b4093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaivePTQ(\n",
       "  (conv1): Conv2d(\n",
       "    3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(\n",
       "    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(\n",
       "    32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn2): BatchNorm2d(\n",
       "    64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(\n",
       "    in_features=4096, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_bias = NaivePTQ().to(device)\n",
    "net_bias.load_state_dict(model.state_dict())\n",
    "net_bias.eval()\n",
    "\n",
    "net_bias.qconfig = torch.ao.quantization.default_qconfig\n",
    "torch.ao.quantization.prepare(net_bias, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b051481f-125c-419c-9e8d-fa214a205d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FP32(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_ref = FP32().to(device)\n",
    "fp32_ref.load_state_dict(model.state_dict())\n",
    "fp32_ref.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cb8495f9-e51e-4d20-9bb7-193a8e888cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_bias = bias_correction_pre_convert(\n",
    "    fp32_ref,\n",
    "    net_bias,\n",
    "    calib_loader\n",
    ")\n",
    "\n",
    "net_bias = torch.ao.quantization.convert(net_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "89164ad9-9142-44bd-8590-01f914dd853e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ with Bias Correction --\n",
      "PTQ with Bias Correction evaluation finished in 0.4729s\n",
      "PTQ with Bias Correction Test Accuracy: 0.7142\n",
      "PTQ with Bias Correction Model Size: 0.5307MB\n",
      "PTQ with Bias Correction Inference Latency: 0.0471ms\n",
      "PTQ with Bias Correction Inference Throughput per second: 21248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(net_bias, model_name=\"PTQ with Bias Correction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96027df3-c1bc-491c-a671-93b0f68270f5",
   "metadata": {},
   "source": [
    "# PTQ + BN Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "71b9501a-1be8-4471-b1a6-bbd25a7e53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTQ_BN_Folding(FP32):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)  # (Fusion will replace conv1+bn1+relu1 with a single ConvReLU module / same with conv2+bn2+relu2)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7a6ccb74-0edf-48c7-9b81-fba9c5cb2f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights from the trained BN model\n",
    "net_ptq_fold = PTQ_BN_Folding().to(device)\n",
    "net_ptq_fold.load_state_dict(model.state_dict())\n",
    "net_ptq_fold.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b6a29bdc-d9fe-479d-9725-5cf61223c4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (bn1): Identity()\n",
       "  (relu1): Identity()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): ConvReLU2d(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (bn2): Identity()\n",
       "  (relu2): Identity()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fusing Conv2d + BatchNorm2d + ReLU\n",
    "torch.ao.quantization.fuse_modules(\n",
    "    net_ptq_fold, \n",
    "    [['conv1', 'bn1', 'relu1'], ['conv2', 'bn2', 'relu2']], \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "49035f38-bd9e-4eed-bc55-9d5e1772b1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn1): Identity()\n",
       "  (relu1): Identity()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): ConvReLU2d(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (bn2): Identity()\n",
       "  (relu2): Identity()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(\n",
       "    in_features=4096, out_features=128, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare (Insert Observers)\n",
    "net_ptq_fold.qconfig = torch.ao.quantization.default_qconfig\n",
    "torch.ao.quantization.prepare(net_ptq_fold, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9edba52f-f933-4a31-8ddc-a8d749ffa592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1007937453687191,\n",
       " 0.96,\n",
       " 0.014840999967418611,\n",
       " 0.1159453122454579,\n",
       " 8624.755763156561)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calibrate\n",
    "evaluate(net_ptq_fold, calib_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "62d99071-7117-403a-ba9e-ee1e8d42cf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTQ_BN_Folding(\n",
       "  (conv1): QuantizedConvReLU2d(3, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0860326960682869, zero_point=0, padding=(1, 1))\n",
       "  (bn1): Identity()\n",
       "  (relu1): Identity()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07265675067901611, zero_point=0, padding=(1, 1))\n",
       "  (bn2): Identity()\n",
       "  (relu2): Identity()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): QuantizedLinear(in_features=4096, out_features=128, scale=1.0024625062942505, zero_point=91, qscheme=torch.per_tensor_affine)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.6121194362640381, zero_point=72, qscheme=torch.per_tensor_affine)\n",
       "  (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual quantization\n",
    "torch.ao.quantization.convert(net_ptq_fold, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "841a1839-ad20-45f1-8c12-e85b8f158319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating PTQ with BN Folding --\n",
      "PTQ with BN Folding evaluation finished in 0.4084s\n",
      "PTQ with BN Folding Test Accuracy: 0.7149\n",
      "PTQ with BN Folding Model Size: 0.5265MB\n",
      "PTQ with BN Folding Inference Latency: 0.0406ms\n",
      "PTQ with BN Folding Inference Throughput per second: 24605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "test(net_ptq_fold, model_name=\"PTQ with BN Folding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b5703-84d3-42f0-b0ae-08e5e49aa5a8",
   "metadata": {},
   "source": [
    "# Mixed Precision PTQ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f654a54-38cc-4b42-b6c8-9284b4b0dce9",
   "metadata": {},
   "source": [
    "## Calculate Layerwise Error / Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "11713049-fa16-46f8-89ae-0c74718a4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_sensitivity(fp32_model, quantized_model, loader, num_batches=None):\n",
    "    fp32_model.eval()\n",
    "    quantized_model.eval()\n",
    "    \n",
    "    total_squared_errors = {} \n",
    "    total_counts = {}\n",
    "    \n",
    "    # temp - cleaned after every batch\n",
    "    batch_fp32_out = {}\n",
    "    batch_quant_out = {}\n",
    "    \n",
    "    target_prefixes = (\"conv\", \"fc\")\n",
    "\n",
    "    def make_hook(storage_dict, layer_name):\n",
    "        def hook(module, inp, out):\n",
    "            if hasattr(out, \"is_quantized\") and out.is_quantized:\n",
    "                out = out.dequantize()\n",
    "            storage_dict[layer_name] = out.detach().cpu()\n",
    "        return hook\n",
    "\n",
    "    handles = []\n",
    "    \n",
    "    for name, mod in fp32_model.named_modules():\n",
    "        if name.startswith(target_prefixes):\n",
    "            handles.append(mod.register_forward_hook(make_hook(batch_fp32_out, name)))\n",
    "            \n",
    "    for name, mod in quantized_model.named_modules():\n",
    "        if name.startswith(target_prefixes):\n",
    "            handles.append(mod.register_forward_hook(make_hook(batch_quant_out, name)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(loader):\n",
    "            if num_batches and i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            x = x.to(device)\n",
    "            \n",
    "            batch_fp32_out.clear()\n",
    "            batch_quant_out.clear()\n",
    "            \n",
    "            fp32_model(x)\n",
    "            quantized_model(x)\n",
    "            \n",
    "            for name in batch_fp32_out.keys():\n",
    "                if name in batch_quant_out:\n",
    "                    out_f = batch_fp32_out[name]  # tensor\n",
    "                    out_q = batch_quant_out[name]\n",
    "                    \n",
    "                    batch_squared_error = (out_f - out_q).pow(2).sum().item()\n",
    "                    batch_count = out_f.numel()\n",
    "                    \n",
    "                    total_squared_errors[name] = total_squared_errors.get(name, 0.0) + batch_squared_error\n",
    "                    total_counts[name] = total_counts.get(name, 0) + batch_count\n",
    "\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    final_sensitivity = {}\n",
    "    for name in total_squared_errors:\n",
    "        final_sensitivity[name] = total_squared_errors[name] / total_counts[name]\n",
    "\n",
    "    return final_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c78ea7e9-ad0a-4c44-99b8-f93e5957505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting robust sensitivity analysis...\n",
      "\n",
      "Layer-wise MSE Error (Averaged over batches):\n",
      "conv2: 5.818714\n",
      "conv1: 0.210803\n",
      "fc2: 0.177673\n",
      "fc1: 0.105543\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Starting robust sensitivity analysis...\")\n",
    "sensitivity = get_layer_sensitivity(\n",
    "    model, \n",
    "    net_ptq_fold, \n",
    "    val_loader,\n",
    "    num_batches=20\n",
    ")\n",
    "\n",
    "print(\"\\nLayer-wise MSE Error (Averaged over batches):\")\n",
    "sorted_layers = sorted(sensitivity.items(), key=lambda item: item[1], reverse=True)\n",
    "for name, score in sorted_layers:\n",
    "    print(f\"{name}: {score:.6f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(3, 5))\n",
    "plt.bar(sensitivity.keys(), sensitivity.values(), color='skyblue')\n",
    "plt.title(\"Layerwise Quantization MSE\")\n",
    "plt.ylabel(\"MSE Loss per Batch\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6cf12-7f25-4ee0-944a-dd910050b863",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b818c54-f5bc-43f7-8a81-926d266b6ba5",
   "metadata": {},
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "350afe73-ef3e-47ce-9152-65ade1f73b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating FP32 Baseline --\n",
      "FP32 Baseline evaluation finished in 1.4326s\n",
      "FP32 Baseline Test Accuracy: 0.7191\n",
      "FP32 Baseline Model Size: 2.0857MB\n",
      "FP32 Baseline Inference Latency: 0.1426ms\n",
      "FP32 Baseline Inference Throughput per second: 7014\n",
      "\n",
      "-- Evaluating PTQ Naive --\n",
      "PTQ Naive evaluation finished in 0.5202s\n",
      "PTQ Naive Test Accuracy: 0.7142\n",
      "PTQ Naive Model Size: 0.5307MB\n",
      "PTQ Naive Inference Latency: 0.0518ms\n",
      "PTQ Naive Inference Throughput per second: 19315\n",
      "\n",
      "-- Evaluating PTQ with Bias Correction --\n",
      "PTQ with Bias Correction evaluation finished in 0.4844s\n",
      "PTQ with Bias Correction Test Accuracy: 0.7142\n",
      "PTQ with Bias Correction Model Size: 0.5307MB\n",
      "PTQ with Bias Correction Inference Latency: 0.0482ms\n",
      "PTQ with Bias Correction Inference Throughput per second: 20745\n",
      "\n",
      "-- Evaluating PTQ with BN Folding --\n",
      "PTQ with BN Folding evaluation finished in 0.4100s\n",
      "PTQ with BN Folding Test Accuracy: 0.7149\n",
      "PTQ with BN Folding Model Size: 0.5265MB\n",
      "PTQ with BN Folding Inference Latency: 0.0408ms\n",
      "PTQ with BN Folding Inference Throughput per second: 24508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, model_name=\"FP32 Baseline\")\n",
    "test(net_ptq_naive, model_name=\"PTQ Naive\")\n",
    "test(net_bias, model_name=\"PTQ with Bias Correction\")\n",
    "test(net_ptq_fold, model_name=\"PTQ with BN Folding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ded07fa3-df12-4aad-a913-3a4724ef191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 latency: 0.1481ms\n",
      "PTQ Naive latency: 0.0518ms\n",
      "PTQ Bias Correction latency: 0.0473ms\n",
      "PTQ BN Folding latency: 0.0413ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"FP32 latency: {calculating_avg_latency(model):.4f}ms\")\n",
    "print(f\"PTQ Naive latency: {calculating_avg_latency(net_ptq_naive):.4f}ms\")\n",
    "print(f\"PTQ Bias Correction latency: {calculating_avg_latency(net_bias):.4f}ms\")\n",
    "print(f\"PTQ BN Folding latency: {calculating_avg_latency(net_ptq_fold):.4f}ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
